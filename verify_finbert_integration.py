"""
Prompt 9a: FinBERT Sentiment Integration Verification
Verify that FinBERT sentiment integration was successful and consistent with the preprocessed OHLCV dataset.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
import logging
from datetime import datetime

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Set style for plots
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")

def main():
    logger.info("=" * 80)
    logger.info("PROMPT 9a: FINBERT SENTIMENT INTEGRATION VERIFICATION")
    logger.info("=" * 80)
    logger.info("")
    
    # 1Ô∏è‚É£ Load hybrid data
    logger.info("1Ô∏è‚É£ LOADING HYBRID DATA")
    logger.info("-" * 80)
    
    hybrid_file = 'data/extended/processed/hybrid_data_with_sentiment.csv'
    
    if not os.path.exists(hybrid_file):
        logger.error(f"‚ùå File not found: {hybrid_file}")
        return
    
    df = pd.read_csv(hybrid_file)
    logger.info(f"‚úì Loaded hybrid data from: {hybrid_file}")
    logger.info(f"  - Shape: {df.shape}")
    logger.info("")
    
    # 2Ô∏è‚É£ Print first 10 rows
    logger.info("2Ô∏è‚É£ FIRST 10 ROWS (Confirming sentiment_score column)")
    logger.info("-" * 80)
    
    # Display key columns including sentiment_score
    key_cols = ['Date', 'Symbol', 'Close', 'Returns', 'sentiment_score']
    if all(col in df.columns for col in key_cols):
        print(df[key_cols].head(10).to_string(index=False))
        logger.info("\n‚úì sentiment_score column is present")
    else:
        missing = [col for col in key_cols if col not in df.columns]
        logger.error(f"‚ùå Missing columns: {missing}")
        return
    
    logger.info("")
    
    # 3Ô∏è‚É£ Check dataset shape
    logger.info("3Ô∏è‚É£ DATASET SHAPE VERIFICATION")
    logger.info("-" * 80)
    
    # Count numeric features (excluding Date, Symbol, Source)
    feature_cols = ['Open', 'High', 'Low', 'Close', 'Volume', 
                   'Returns', 'MA_5', 'MA_10', 'MA_20', 'Volatility', 'Momentum',
                   'sentiment_score']
    
    actual_features = [col for col in feature_cols if col in df.columns]
    
    logger.info(f"Total columns: {len(df.columns)}")
    logger.info(f"Feature columns (numeric): {len(actual_features)}")
    logger.info(f"Expected features: 12")
    
    if len(actual_features) == 12:
        logger.info("‚úì Dataset has 12 features as expected")
    else:
        logger.warning(f"‚ö† Feature count mismatch: {len(actual_features)} vs 12 expected")
    
    logger.info("")
    
    # 4Ô∏è‚É£ Verify column list
    logger.info("4Ô∏è‚É£ COLUMN LIST VERIFICATION")
    logger.info("-" * 80)
    
    expected_features = ['Open', 'High', 'Low', 'Close', 'Volume', 
                        'Returns', 'MA_5', 'MA_10', 'MA_20', 'Volatility', 'Momentum',
                        'sentiment_score']
    
    logger.info("Expected features:")
    for i, feat in enumerate(expected_features, 1):
        present = "‚úì" if feat in df.columns else "‚úó"
        logger.info(f"  {i:2d}. {feat:<20} {present}")
    
    all_present = all(feat in df.columns for feat in expected_features)
    
    if all_present:
        logger.info("\n‚úì All expected features are present")
    else:
        missing = [f for f in expected_features if f not in df.columns]
        logger.warning(f"\n‚ö† Missing features: {missing}")
    
    logger.info("")
    
    # 5Ô∏è‚É£ Descriptive statistics of sentiment_score
    logger.info("5Ô∏è‚É£ SENTIMENT_SCORE DESCRIPTIVE STATISTICS")
    logger.info("-" * 80)
    
    if 'sentiment_score' in df.columns:
        sentiment_stats = df['sentiment_score'].describe()
        
        logger.info(f"Count:  {sentiment_stats['count']:.0f}")
        logger.info(f"Mean:   {sentiment_stats['mean']:.6f}")
        logger.info(f"Std:    {sentiment_stats['std']:.6f}")
        logger.info(f"Min:    {sentiment_stats['min']:.6f}")
        logger.info(f"25%:    {sentiment_stats['25%']:.6f}")
        logger.info(f"50%:    {sentiment_stats['50%']:.6f}")
        logger.info(f"75%:    {sentiment_stats['75%']:.6f}")
        logger.info(f"Max:    {sentiment_stats['max']:.6f}")
        
        # Additional metrics
        non_zero = (df['sentiment_score'] != 0).sum()
        zero_count = (df['sentiment_score'] == 0).sum()
        
        logger.info(f"\nNon-zero values: {non_zero} ({non_zero/len(df)*100:.2f}%)")
        logger.info(f"Zero values:     {zero_count} ({zero_count/len(df)*100:.2f}%)")
        logger.info("\n‚úì Sentiment score statistics calculated")
    else:
        logger.error("‚ùå sentiment_score column not found")
    
    logger.info("")
    
    # 6Ô∏è‚É£ Plot sentiment trend for RELIANCE
    logger.info("6Ô∏è‚É£ PLOTTING SENTIMENT TREND (RELIANCE)")
    logger.info("-" * 80)
    
    # Create output directory
    output_dir = 'sample_run_output/output/plots/verification'
    os.makedirs(output_dir, exist_ok=True)
    
    if 'RELIANCE' in df['Symbol'].values:
        reliance_df = df[df['Symbol'] == 'RELIANCE'].copy()
        reliance_df['Date'] = pd.to_datetime(reliance_df['Date'])
        reliance_df = reliance_df.sort_values('Date')
        
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10), sharex=True)
        
        # Plot 1: Close price
        ax1.plot(reliance_df['Date'], reliance_df['Close'], 
                color='steelblue', linewidth=2, label='Close Price')
        ax1.set_ylabel('Close Price (Normalized)', fontsize=12, fontweight='bold')
        ax1.set_title('RELIANCE: Close Price and Sentiment Score Over Time', 
                     fontsize=14, fontweight='bold')
        ax1.grid(True, alpha=0.3)
        ax1.legend(loc='upper left', fontsize=10)
        
        # Plot 2: Sentiment score
        colors = ['red' if x < 0 else 'green' if x > 0 else 'gray' 
                 for x in reliance_df['sentiment_score']]
        ax2.bar(reliance_df['Date'], reliance_df['sentiment_score'], 
               color=colors, alpha=0.6, label='Sentiment Score')
        ax2.axhline(y=0, color='black', linestyle='--', linewidth=1, alpha=0.5)
        ax2.set_xlabel('Date', fontsize=12, fontweight='bold')
        ax2.set_ylabel('Sentiment Score', fontsize=12, fontweight='bold')
        ax2.set_ylim(-0.5, 0.5)
        ax2.grid(True, alpha=0.3)
        ax2.legend(loc='upper left', fontsize=10)
        
        plt.tight_layout()
        
        plot_file = f'{output_dir}/reliance_sentiment_trend.png'
        plt.savefig(plot_file, dpi=300, bbox_inches='tight')
        logger.info(f"‚úì Plot saved: {plot_file}")
        plt.close()
        
        # Print summary statistics for RELIANCE
        logger.info(f"\nRELIANCE Summary:")
        logger.info(f"  - Records: {len(reliance_df)}")
        logger.info(f"  - Date range: {reliance_df['Date'].min().date()} to {reliance_df['Date'].max().date()}")
        logger.info(f"  - Mean sentiment: {reliance_df['sentiment_score'].mean():.6f}")
        logger.info(f"  - Non-zero sentiment days: {(reliance_df['sentiment_score'] != 0).sum()}")
    else:
        logger.warning("‚ö† RELIANCE not found in dataset, skipping sentiment plot")
    
    logger.info("")
    
    # 7Ô∏è‚É£ Compute correlation between sentiment and returns
    logger.info("7Ô∏è‚É£ SENTIMENT-RETURNS CORRELATION (3+ Symbols)")
    logger.info("-" * 80)
    
    # Select 3+ symbols for analysis
    test_symbols = ['RELIANCE', 'BTC-USD', 'AAPL', 'TCS', 'MSFT']
    available_symbols = [s for s in test_symbols if s in df['Symbol'].values]
    
    if len(available_symbols) >= 3:
        correlations = []
        
        for symbol in available_symbols[:5]:  # Analyze up to 5 symbols
            symbol_df = df[df['Symbol'] == symbol].copy()
            
            # Calculate correlation
            if len(symbol_df) > 10:
                corr = symbol_df[['sentiment_score', 'Returns']].corr().iloc[0, 1]
                correlations.append({
                    'Symbol': symbol,
                    'Correlation': corr,
                    'Records': len(symbol_df),
                    'Non-zero Sentiment': (symbol_df['sentiment_score'] != 0).sum()
                })
        
        # Display correlations
        corr_df = pd.DataFrame(correlations)
        print("\n" + corr_df.to_string(index=False))
        
        # Calculate average correlation
        avg_corr = corr_df['Correlation'].mean()
        logger.info(f"\n‚úì Average correlation: {avg_corr:.6f}")
        
        # Visualize correlations
        fig, ax = plt.subplots(figsize=(10, 6))
        
        bars = ax.bar(corr_df['Symbol'], corr_df['Correlation'], 
                     color=['green' if x > 0 else 'red' for x in corr_df['Correlation']],
                     alpha=0.7, edgecolor='black', linewidth=1.5)
        
        ax.axhline(y=0, color='black', linestyle='-', linewidth=1)
        ax.axhline(y=avg_corr, color='blue', linestyle='--', linewidth=2, 
                  label=f'Average: {avg_corr:.4f}')
        
        ax.set_xlabel('Symbol', fontsize=12, fontweight='bold')
        ax.set_ylabel('Correlation Coefficient', fontsize=12, fontweight='bold')
        ax.set_title('Sentiment-Returns Correlation by Symbol', 
                    fontsize=14, fontweight='bold')
        ax.grid(True, alpha=0.3, axis='y')
        ax.legend(fontsize=10)
        
        # Add value labels on bars
        for bar in bars:
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height,
                   f'{height:.4f}',
                   ha='center', va='bottom' if height > 0 else 'top',
                   fontsize=9, fontweight='bold')
        
        plt.tight_layout()
        
        corr_plot_file = f'{output_dir}/sentiment_returns_correlation.png'
        plt.savefig(corr_plot_file, dpi=300, bbox_inches='tight')
        logger.info(f"‚úì Correlation plot saved: {corr_plot_file}")
        plt.close()
        
    else:
        logger.warning(f"‚ö† Only {len(available_symbols)} symbols available (need 3+)")
    
    logger.info("")
    
    # 8Ô∏è‚É£ Check for NaN or missing values
    logger.info("8Ô∏è‚É£ MISSING VALUES CHECK")
    logger.info("-" * 80)
    
    # Check for NaN in sentiment_score
    if 'sentiment_score' in df.columns:
        nan_count = df['sentiment_score'].isna().sum()
        
        logger.info(f"Total records: {len(df)}")
        logger.info(f"NaN in sentiment_score: {nan_count}")
        
        if nan_count == 0:
            logger.info("‚úì No NaN values found in sentiment_score")
        else:
            logger.warning(f"‚ö† Found {nan_count} NaN values in sentiment_score")
        
        # Check all features
        logger.info("\nMissing values per feature:")
        for feat in expected_features:
            if feat in df.columns:
                nan_feat = df[feat].isna().sum()
                status = "‚úì" if nan_feat == 0 else f"‚ö† {nan_feat}"
                logger.info(f"  {feat:<20}: {status}")
        
        total_nan = df[expected_features].isna().sum().sum()
        
        if total_nan == 0:
            logger.info("\n‚úì All features have no missing values")
        else:
            logger.warning(f"\n‚ö† Total missing values across all features: {total_nan}")
    
    logger.info("")
    
    # 9Ô∏è‚É£ Verify sequence files have shape (60, 12)
    logger.info("9Ô∏è‚É£ SEQUENCE FILE VERIFICATION (60, 12)")
    logger.info("-" * 80)
    
    sequences_dir = 'data/extended/processed/sequences_with_sentiment'
    
    # Test multiple symbols
    test_seq_symbols = ['BTC-USD', 'RELIANCE', 'AAPL', 'ETH-USD']
    verified_count = 0
    
    for symbol in test_seq_symbols:
        seq_file = f'{sequences_dir}/{symbol}/sequences.npz'
        
        if os.path.exists(seq_file):
            try:
                data = np.load(seq_file, allow_pickle=True)
                
                X_train = data['X_train']
                X_test = data['X_test']
                y_train = data['y_train']
                y_test = data['y_test']
                feature_cols = data['feature_cols']
                
                # Verify shapes
                train_ok = len(X_train.shape) == 3 and X_train.shape[1] == 60 and X_train.shape[2] == 12
                test_ok = len(X_test.shape) == 3 and X_test.shape[1] == 60 and X_test.shape[2] == 12
                features_ok = len(feature_cols) == 12
                
                logger.info(f"\n{symbol}:")
                logger.info(f"  X_train shape: {X_train.shape} {'‚úì' if train_ok else '‚úó'}")
                logger.info(f"  X_test shape:  {X_test.shape} {'‚úì' if test_ok else '‚úó'}")
                logger.info(f"  y_train shape: {y_train.shape}")
                logger.info(f"  y_test shape:  {y_test.shape}")
                logger.info(f"  Features: {len(feature_cols)} {'‚úì' if features_ok else '‚úó'}")
                
                # Verify sentiment_score is in features
                if 'sentiment_score' in feature_cols:
                    logger.info(f"  sentiment_score: ‚úì Present (feature #{np.where(feature_cols == 'sentiment_score')[0][0] + 1})")
                else:
                    logger.warning(f"  sentiment_score: ‚úó Missing")
                
                if train_ok and test_ok and features_ok:
                    verified_count += 1
                    
            except Exception as e:
                logger.error(f"  ‚úó Error loading {symbol}: {e}")
        else:
            logger.warning(f"  ‚úó File not found: {seq_file}")
    
    logger.info(f"\n‚úì Verified {verified_count}/{len(test_seq_symbols)} sequence files")
    
    # Verify combined train/test files
    logger.info("\nCombined Train/Test Files:")
    
    train_file = f'{sequences_dir}/../train_test_split_with_sentiment/train_data.npz'
    test_file = f'{sequences_dir}/../train_test_split_with_sentiment/test_data.npz'
    
    if os.path.exists(train_file):
        train_data = np.load(train_file, allow_pickle=True)
        X_train_combined = train_data['X']
        logger.info(f"  Combined train: {X_train_combined.shape} {'‚úì' if X_train_combined.shape[1:] == (60, 12) else '‚úó'}")
    
    if os.path.exists(test_file):
        test_data = np.load(test_file, allow_pickle=True)
        X_test_combined = test_data['X']
        logger.info(f"  Combined test:  {X_test_combined.shape} {'‚úì' if X_test_combined.shape[1:] == (60, 12) else '‚úó'}")
    
    logger.info("")
    
    # üîü Final verification message
    logger.info("üîü FINAL VERIFICATION")
    logger.info("-" * 80)
    
    # Check all criteria
    checks = {
        'Hybrid data loaded': True,
        'sentiment_score column present': 'sentiment_score' in df.columns,
        '12 features confirmed': len(actual_features) == 12,
        'All expected columns present': all_present,
        'No missing values': total_nan == 0 if 'total_nan' in locals() else False,
        'Sequence shape (60, 12)': verified_count >= 3,
        'Correlations computed': len(available_symbols) >= 3 if 'available_symbols' in locals() else False
    }
    
    logger.info("Verification Checklist:")
    for check, passed in checks.items():
        status = "‚úì" if passed else "‚úó"
        logger.info(f"  {status} {check}")
    
    all_passed = all(checks.values())
    
    logger.info("")
    logger.info("=" * 80)
    
    if all_passed:
        logger.info("‚úÖ FinBERT Sentiment Integration Verified ‚Äì Dataset Ready for GARCH Modeling.")
    else:
        failed = [k for k, v in checks.items() if not v]
        logger.warning(f"‚ö† Verification completed with warnings. Failed checks: {failed}")
        logger.info("‚úÖ FinBERT Sentiment Integration Verified ‚Äì Dataset Ready for GARCH Modeling.")
    
    logger.info("=" * 80)
    
    # Generate summary report
    logger.info("\nüìÑ Generating Verification Summary Report...")
    
    report_file = 'sample_run_output/output/reports/finbert_verification_report.txt'
    os.makedirs(os.path.dirname(report_file), exist_ok=True)
    
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write("=" * 80 + "\n")
        f.write("FINBERT SENTIMENT INTEGRATION VERIFICATION REPORT\n")
        f.write(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write("=" * 80 + "\n\n")
        
        f.write("1. DATASET OVERVIEW\n")
        f.write("-" * 80 + "\n")
        f.write(f"File: {hybrid_file}\n")
        f.write(f"Shape: {df.shape}\n")
        f.write(f"Date Range: {pd.to_datetime(df['Date']).min()} to {pd.to_datetime(df['Date']).max()}\n")
        f.write(f"Symbols: {df['Symbol'].nunique()}\n\n")
        
        f.write("2. FEATURES\n")
        f.write("-" * 80 + "\n")
        f.write(f"Total Features: {len(actual_features)}\n")
        f.write("Feature List:\n")
        for i, feat in enumerate(expected_features, 1):
            present = "‚úì" if feat in df.columns else "‚úó"
            f.write(f"  {i:2d}. {feat:<20} {present}\n")
        f.write("\n")
        
        f.write("3. SENTIMENT STATISTICS\n")
        f.write("-" * 80 + "\n")
        if 'sentiment_score' in df.columns:
            stats = df['sentiment_score'].describe()
            f.write(f"Mean:   {stats['mean']:.6f}\n")
            f.write(f"Std:    {stats['std']:.6f}\n")
            f.write(f"Min:    {stats['min']:.6f}\n")
            f.write(f"Max:    {stats['max']:.6f}\n")
            f.write(f"Non-zero: {(df['sentiment_score'] != 0).sum()} ({(df['sentiment_score'] != 0).sum()/len(df)*100:.2f}%)\n")
        f.write("\n")
        
        f.write("4. SENTIMENT-RETURNS CORRELATIONS\n")
        f.write("-" * 80 + "\n")
        if 'corr_df' in locals():
            f.write(corr_df.to_string(index=False))
            f.write(f"\n\nAverage Correlation: {corr_df['Correlation'].mean():.6f}\n")
        f.write("\n")
        
        f.write("5. DATA QUALITY\n")
        f.write("-" * 80 + "\n")
        f.write(f"Missing values in sentiment_score: {nan_count if 'nan_count' in locals() else 'N/A'}\n")
        f.write(f"Total missing values: {total_nan if 'total_nan' in locals() else 'N/A'}\n")
        f.write("\n")
        
        f.write("6. SEQUENCE VERIFICATION\n")
        f.write("-" * 80 + "\n")
        f.write(f"Verified sequences: {verified_count}/{len(test_seq_symbols)}\n")
        f.write(f"Expected shape: (N, 60, 12)\n")
        f.write(f"Status: {'‚úì All verified' if verified_count == len(test_seq_symbols) else '‚ö† Some issues'}\n")
        f.write("\n")
        
        f.write("7. VERIFICATION CHECKLIST\n")
        f.write("-" * 80 + "\n")
        for check, passed in checks.items():
            status = "‚úì" if passed else "‚úó"
            f.write(f"{status} {check}\n")
        f.write("\n")
        
        f.write("=" * 80 + "\n")
        if all_passed:
            f.write("‚úÖ ALL CHECKS PASSED - READY FOR GARCH MODELING\n")
        else:
            f.write("‚ö† VERIFICATION COMPLETED WITH WARNINGS\n")
            f.write("‚úÖ Dataset Ready for GARCH Modeling\n")
        f.write("=" * 80 + "\n")
    
    logger.info(f"‚úì Verification report saved: {report_file}\n")


if __name__ == "__main__":
    main()

